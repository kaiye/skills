#!/usr/bin/env python3
"""
æŠ–éŸ³é•¿è§†é¢‘æœ¬åœ°å¤„ç†å·¥å…·

åŠŸèƒ½ï¼š
1. è§£ææŠ–éŸ³åˆ†äº«é“¾æ¥è·å–æ— æ°´å°è§†é¢‘
2. ä¸‹è½½è§†é¢‘åˆ°æœ¬åœ°
3. é•¿è§†é¢‘è‡ªåŠ¨åˆ‡åˆ†ä¸ºç‰‡æ®µ
4. æ”¯æŒä¸¤ç§æ–‡æ¡ˆæå–æ–¹å¼ï¼š
   - ASR è¯­éŸ³è¯†åˆ«ï¼ˆé»˜è®¤ï¼‰
   - OCR ç¡¬å­—å¹•æå–ï¼ˆæ›´å‡†ç¡®ï¼Œéœ€è¦è§†é¢‘æœ‰å­—å¹•ï¼‰
5. åˆå¹¶æ‰€æœ‰è¯†åˆ«ç»“æœ

ä½¿ç”¨æ–¹æ³•ï¼š
    python douyin_video_processor.py "æŠ–éŸ³åˆ†äº«é“¾æ¥" [é€‰é¡¹]
    python douyin_video_processor.py "æŠ–éŸ³åˆ†äº«é“¾æ¥" --ocr  # ä½¿ç”¨OCRæå–å­—å¹•

ç¯å¢ƒå˜é‡ï¼š
    DASHSCOPE_API_KEY: é˜¿é‡Œäº‘ç™¾ç‚¼APIå¯†é’¥ï¼ˆASRæ¨¡å¼éœ€è¦ï¼‰
"""

from __future__ import annotations

import subprocess
import sys

# ============================================================
# ä¾èµ–è‡ªåŠ¨æ£€æµ‹ä¸å®‰è£…
# ============================================================
def ensure_dependencies(use_ocr=False):
    """æ£€æŸ¥å¹¶å®‰è£…ç¼ºå¤±çš„ä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œæ—¶è‡ªåŠ¨å®‰è£…ï¼‰"""
    # åŸºç¡€ä¾èµ–ï¼ˆASR æ¨¡å¼ï¼‰
    REQUIRED = {
        "dashscope": "dashscope",
        "requests": "requests",
    }

    # OCR æ¨¡å¼é¢å¤–ä¾èµ–
    if use_ocr:
        REQUIRED.update({
            "cv2": "opencv-python",
            "numpy": "numpy",
            "paddleocr": "paddleocr",
            "paddle": "paddlepaddle",
        })

    missing = []
    for module, package in REQUIRED.items():
        try:
            __import__(module)
        except ImportError:
            missing.append(package)

    if missing:
        print(f"ğŸ“¦ æ­£åœ¨å®‰è£…ç¼ºå¤±çš„ä¾èµ–: {missing}")
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", "-q"] + missing,
        )
        print("âœ… ä¾èµ–å®‰è£…å®Œæˆ\n")

# æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ OCR æ¨¡å¼ï¼ˆåœ¨ argparse ä¹‹å‰ç®€å•æ£€æŸ¥ï¼‰
_use_ocr = "--ocr" in sys.argv
ensure_dependencies(use_ocr=_use_ocr)
# ============================================================

import os
import re
import json
import argparse
import tempfile
import shutil
from pathlib import Path
from typing import Optional, List, Dict, Tuple
from urllib import request
from http import HTTPStatus
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from difflib import SequenceMatcher
from multiprocessing import Manager

# ç°åœ¨å¯ä»¥å®‰å…¨å¯¼å…¥æ‰€æœ‰ä¾èµ–
import requests
import dashscope

# OCR ç›¸å…³ä¾èµ–å»¶è¿Ÿå¯¼å…¥
if _use_ocr:
    import cv2
    import numpy as np
    from paddleocr import PaddleOCR

# è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿç§»åŠ¨ç«¯è®¿é—®
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) EdgiOS/121.0.2277.107 Version/17.0 Mobile/15E148 Safari/604.1'
}

# é»˜è®¤é…ç½®
DEFAULT_MODEL = "paraformer-v2"
DEFAULT_SEGMENT_DURATION = 300  # é»˜è®¤5åˆ†é’Ÿä¸€ä¸ªç‰‡æ®µ
MAX_PARALLEL_TRANSCRIPTIONS = 3  # æœ€å¤§å¹¶è¡Œè½¬å½•æ•°

# OCR é…ç½®
OCR_FRAME_INTERVAL = 1.0  # æ¯éš”å¤šå°‘ç§’é‡‡æ ·ä¸€å¸§
OCR_SIMILARITY_THRESHOLD = 0.85  # æ–‡æœ¬ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œç”¨äºå»é‡
OCR_SEGMENT_DURATION = 60  # OCR æ¨¡å¼è§†é¢‘åˆ‡åˆ†æ—¶é•¿ï¼ˆç§’ï¼‰
OCR_PARALLEL_WORKERS = 4  # OCR å¹¶è¡Œè¿›ç¨‹æ•°


def _process_segment_ocr(args):
    """
    å¤„ç†å•ä¸ªè§†é¢‘ç‰‡æ®µçš„ OCRï¼ˆç”¨äºå¤šè¿›ç¨‹ï¼‰

    Args:
        args: (segment_path, segment_index, time_offset, lang, subtitle_area)

    Returns:
        (segment_index, subtitles_list)
    """
    segment_path, segment_index, time_offset, lang, subtitle_area = args

    # æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹åˆ›å»º OCR å®ä¾‹
    from paddleocr import PaddleOCR
    ocr = PaddleOCR(lang=lang)

    # æ‰“å¼€è§†é¢‘
    cap = cv2.VideoCapture(str(segment_path))
    if not cap.isOpened():
        return (segment_index, [])

    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps if fps > 0 else 0

    # å­—å¹•åŒºåŸŸ
    sub_area = subtitle_area or (0.70, 0.95, 0.05, 0.95)

    def extract_text(frame):
        """ä»å¸§ä¸­æå–æ–‡å­—"""
        h, w = frame.shape[:2]
        # è£å‰ªå­—å¹•åŒºåŸŸ
        y1, y2 = int(h * sub_area[0]), int(h * sub_area[1])
        x1, x2 = int(w * sub_area[2]), int(w * sub_area[3])
        region = frame[y1:y2, x1:x2]

        # é¢„å¤„ç†
        gray = cv2.cvtColor(region, cv2.COLOR_BGR2GRAY)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(gray)
        _, binary = cv2.threshold(enhanced, 200, 255, cv2.THRESH_BINARY)
        processed = cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)

        # OCR
        result = ocr.predict(processed)
        if not result or len(result) == 0:
            return ""

        res = result[0]
        texts = [t for t, s in zip(res.get('rec_texts', []), res.get('rec_scores', [])) if s > 0.7]
        return " ".join(texts)

    def is_similar(t1, t2):
        if not t1 or not t2:
            return False
        return SequenceMatcher(None, t1, t2).ratio() >= OCR_SIMILARITY_THRESHOLD

    # é‡‡æ ·å¤„ç†
    subtitles = []
    current_text = ""
    current_start = 0.0
    t = 0.0

    while t < duration:
        cap.set(cv2.CAP_PROP_POS_MSEC, t * 1000)
        ret, frame = cap.read()
        if not ret:
            t += OCR_FRAME_INTERVAL
            continue

        text = extract_text(frame)

        if text and not is_similar(text, current_text):
            if current_text:
                subtitles.append({
                    "start_time": time_offset + current_start,
                    "end_time": time_offset + t,
                    "text": current_text
                })
            current_text = text
            current_start = t
        elif not text and current_text:
            subtitles.append({
                "start_time": time_offset + current_start,
                "end_time": time_offset + t,
                "text": current_text
            })
            current_text = ""

        t += OCR_FRAME_INTERVAL

    if current_text:
        subtitles.append({
            "start_time": time_offset + current_start,
            "end_time": time_offset + duration,
            "text": current_text
        })

    cap.release()
    print(f"   âœ… ç‰‡æ®µ {segment_index + 1} å®Œæˆï¼Œæå– {len(subtitles)} æ¡å­—å¹•")
    return (segment_index, subtitles)


class SubtitleOCR:
    """è§†é¢‘ç¡¬å­—å¹• OCR æå–å™¨"""

    def __init__(
        self,
        use_gpu: bool = False,
        lang: str = 'ch',
        subtitle_area: Optional[Tuple[float, float, float, float]] = None
    ):
        """
        åˆå§‹åŒ– OCR æå–å™¨

        Args:
            use_gpu: æ˜¯å¦ä½¿ç”¨ GPU åŠ é€Ÿ
            lang: OCR è¯­è¨€ï¼Œ'ch' ä¸­æ–‡, 'en' è‹±æ–‡
            subtitle_area: å­—å¹•åŒºåŸŸ (y_start_ratio, y_end_ratio, x_start_ratio, x_end_ratio)
                          ä¾‹å¦‚ (0.75, 0.95, 0.1, 0.9) è¡¨ç¤ºç”»é¢ä¸‹æ–¹ 75%-95% é«˜åº¦ï¼Œå·¦å³å„ç•™ 10%
        """
        # PaddleOCR 3.x API å˜åŒ–ï¼šç§»é™¤äº† show_log, use_angle_cls, use_gpu ç­‰å‚æ•°
        # GPU ä½¿ç”¨ç”± PaddlePaddle æ¡†æ¶è‡ªåŠ¨æ£€æµ‹
        self.ocr = PaddleOCR(lang=lang)
        # é»˜è®¤å­—å¹•åŒºåŸŸï¼šç”»é¢åº•éƒ¨ 70%-95%ï¼Œå·¦å³å„ç•™ 5%
        self.subtitle_area = subtitle_area or (0.70, 0.95, 0.05, 0.95)

    def _crop_subtitle_area(self, frame: np.ndarray) -> np.ndarray:
        """è£å‰ªå­—å¹•åŒºåŸŸ"""
        h, w = frame.shape[:2]
        y_start = int(h * self.subtitle_area[0])
        y_end = int(h * self.subtitle_area[1])
        x_start = int(w * self.subtitle_area[2])
        x_end = int(w * self.subtitle_area[3])
        return frame[y_start:y_end, x_start:x_end]

    def _preprocess_frame(self, frame: np.ndarray) -> np.ndarray:
        """é¢„å¤„ç†å¸§ä»¥æé«˜ OCR å‡†ç¡®ç‡"""
        # è½¬ç°åº¦
        if len(frame.shape) == 3:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        else:
            gray = frame

        # å¢å¼ºå¯¹æ¯”åº¦
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
        enhanced = clahe.apply(gray)

        # äºŒå€¼åŒ–ï¼ˆé’ˆå¯¹ç™½è‰²å­—å¹•ï¼‰
        _, binary = cv2.threshold(enhanced, 200, 255, cv2.THRESH_BINARY)

        # è½¬å› BGRï¼ˆPaddleOCR éœ€è¦ï¼‰
        return cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)

    def _extract_text_from_frame(self, frame: np.ndarray) -> str:
        """ä»å•å¸§æå–æ–‡å­—"""
        # è£å‰ªå­—å¹•åŒºåŸŸ
        subtitle_region = self._crop_subtitle_area(frame)

        # é¢„å¤„ç†
        processed = self._preprocess_frame(subtitle_region)

        # OCR è¯†åˆ« (PaddleOCR 3.x ä½¿ç”¨ predict)
        result = self.ocr.predict(processed)

        if not result or len(result) == 0:
            return ""

        # PaddleOCR 3.x è¿”å›æ ¼å¼: [{'rec_texts': [...], 'rec_scores': [...]}]
        res = result[0]
        rec_texts = res.get('rec_texts', [])
        rec_scores = res.get('rec_scores', [])

        # æå–ç½®ä¿¡åº¦é«˜çš„æ–‡å­—
        texts = []
        for text, score in zip(rec_texts, rec_scores):
            if score > 0.7:  # åªä¿ç•™ç½®ä¿¡åº¦é«˜çš„
                texts.append(text)

        return " ".join(texts)

    def _is_similar(self, text1: str, text2: str, threshold: float = OCR_SIMILARITY_THRESHOLD) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªæ–‡æœ¬æ˜¯å¦ç›¸ä¼¼"""
        if not text1 or not text2:
            return False
        ratio = SequenceMatcher(None, text1, text2).ratio()
        return ratio >= threshold

    def extract_subtitles(
        self,
        video_path: Path,
        frame_interval: float = OCR_FRAME_INTERVAL,
        progress_callback: Optional[callable] = None
    ) -> List[Dict]:
        """
        ä»è§†é¢‘ä¸­æå–å­—å¹•

        Args:
            video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
            frame_interval: å¸§é‡‡æ ·é—´éš”ï¼ˆç§’ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°

        Returns:
            å­—å¹•åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å« start_time, end_time, text
        """
        cap = cv2.VideoCapture(str(video_path))

        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")

        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0

        # è®¡ç®—éœ€è¦å¤„ç†çš„æ—¶é—´ç‚¹
        sample_times = []
        t = 0.0
        while t < duration:
            sample_times.append(t)
            t += frame_interval

        total_samples = len(sample_times)
        print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯: {duration:.1f}ç§’, {fps:.1f}fps, å…±{total_samples}å¸§å¾…å¤„ç†")

        subtitles = []
        current_text = ""
        current_start = 0.0

        for i, current_time in enumerate(sample_times):
            # ç›´æ¥è·³è½¬åˆ°ç›®æ ‡æ—¶é—´ç‚¹ï¼ˆæ¯«ç§’ï¼‰ï¼Œé¿å…é€å¸§è§£ç 
            cap.set(cv2.CAP_PROP_POS_MSEC, current_time * 1000)
            ret, frame = cap.read()

            if not ret:
                continue

            text = self._extract_text_from_frame(frame)

            if progress_callback:
                progress_callback(i + 1, total_samples)

            # æ–‡æœ¬å˜åŒ–æ£€æµ‹
            if text and not self._is_similar(text, current_text):
                if current_text:
                    subtitles.append({
                        "start_time": current_start,
                        "end_time": current_time,
                        "text": current_text
                    })
                current_text = text
                current_start = current_time
            elif not text and current_text:
                subtitles.append({
                    "start_time": current_start,
                    "end_time": current_time,
                    "text": current_text
                })
                current_text = ""

        if current_text:
            subtitles.append({
                "start_time": current_start,
                "end_time": duration,
                "text": current_text
            })

        cap.release()

        print(f"\nâœ… è¯†åˆ«å®Œæˆï¼Œå…±å¤„ç† {len(sample_times)} å¸§ï¼Œæå– {len(subtitles)} æ¡å­—å¹•")

        return subtitles

    def subtitles_to_text(self, subtitles: List[Dict]) -> str:
        """å°†å­—å¹•åˆ—è¡¨åˆå¹¶ä¸ºçº¯æ–‡æœ¬"""
        # å»é‡å¹¶åˆå¹¶
        unique_texts = []
        for sub in subtitles:
            text = sub["text"].strip()
            if text and (not unique_texts or not self._is_similar(text, unique_texts[-1])):
                unique_texts.append(text)

        return "\n".join(unique_texts)

    def subtitles_to_srt(self, subtitles: List[Dict]) -> str:
        """å°†å­—å¹•åˆ—è¡¨è½¬æ¢ä¸º SRT æ ¼å¼"""
        srt_lines = []

        for i, sub in enumerate(subtitles, 1):
            start = self._format_srt_time(sub["start_time"])
            end = self._format_srt_time(sub["end_time"])
            text = sub["text"]

            srt_lines.append(f"{i}")
            srt_lines.append(f"{start} --> {end}")
            srt_lines.append(text)
            srt_lines.append("")

        return "\n".join(srt_lines)

    @staticmethod
    def _format_srt_time(seconds: float) -> str:
        """æ ¼å¼åŒ–ä¸º SRT æ—¶é—´æ ¼å¼"""
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"


class DouyinVideoProcessor:
    """æŠ–éŸ³é•¿è§†é¢‘å¤„ç†å™¨"""

    def __init__(
        self,
        api_key: str = "",
        model: str = DEFAULT_MODEL,
        output_dir: Optional[str] = None,
        segment_duration: int = DEFAULT_SEGMENT_DURATION,
        use_ocr: bool = False,
        ocr_gpu: bool = False
    ):
        self.api_key = api_key
        self.model = model
        self.segment_duration = segment_duration
        self.use_ocr = use_ocr
        self.ocr_gpu = ocr_gpu

        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir:
            self.output_dir = Path(output_dir)
            self.output_dir.mkdir(parents=True, exist_ok=True)
        else:
            self.output_dir = Path(tempfile.mkdtemp(prefix="douyin_"))

        # è®¾ç½®é˜¿é‡Œäº‘ç™¾ç‚¼APIå¯†é’¥ï¼ˆASR æ¨¡å¼éœ€è¦ï¼‰
        if api_key:
            dashscope.api_key = api_key

        # åˆå§‹åŒ– OCRï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self._ocr_engine = None

        print(f"ğŸ“ å·¥ä½œç›®å½•: {self.output_dir}")
        if use_ocr:
            print("ğŸ”¤ æå–æ¨¡å¼: OCR ç¡¬å­—å¹•è¯†åˆ«")
        else:
            print("ğŸ¤ æå–æ¨¡å¼: ASR è¯­éŸ³è¯†åˆ«")

    @property
    def ocr_engine(self) -> SubtitleOCR:
        """å»¶è¿ŸåŠ è½½ OCR å¼•æ“"""
        if self._ocr_engine is None:
            self._ocr_engine = SubtitleOCR(use_gpu=self.ocr_gpu)
        return self._ocr_engine

    def parse_share_url(self, share_text: str) -> Dict:
        """ä»åˆ†äº«æ–‡æœ¬ä¸­æå–æ— æ°´å°è§†é¢‘é“¾æ¥"""
        print("ğŸ” æ­£åœ¨è§£ææŠ–éŸ³åˆ†äº«é“¾æ¥...")

        # æå–åˆ†äº«é“¾æ¥
        urls = re.findall(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',
            share_text
        )
        if not urls:
            raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„åˆ†äº«é“¾æ¥")

        share_url = urls[0]
        share_response = requests.get(share_url, headers=HEADERS)
        video_id = share_response.url.split("?")[0].strip("/").split("/")[-1]
        share_url = f'https://www.iesdouyin.com/share/video/{video_id}'

        # è·å–è§†é¢‘é¡µé¢å†…å®¹
        response = requests.get(share_url, headers=HEADERS)
        response.raise_for_status()

        pattern = re.compile(
            pattern=r"window\._ROUTER_DATA\s*=\s*(.*?)</script>",
            flags=re.DOTALL,
        )
        find_res = pattern.search(response.text)

        if not find_res or not find_res.group(1):
            raise ValueError("ä»HTMLä¸­è§£æè§†é¢‘ä¿¡æ¯å¤±è´¥")

        # è§£æJSONæ•°æ®
        json_data = json.loads(find_res.group(1).strip())
        VIDEO_ID_PAGE_KEY = "video_(id)/page"
        NOTE_ID_PAGE_KEY = "note_(id)/page"

        if VIDEO_ID_PAGE_KEY in json_data["loaderData"]:
            original_video_info = json_data["loaderData"][VIDEO_ID_PAGE_KEY]["videoInfoRes"]
        elif NOTE_ID_PAGE_KEY in json_data["loaderData"]:
            original_video_info = json_data["loaderData"][NOTE_ID_PAGE_KEY]["videoInfoRes"]
        else:
            raise Exception("æ— æ³•ä»JSONä¸­è§£æè§†é¢‘æˆ–å›¾é›†ä¿¡æ¯")

        data = original_video_info["item_list"][0]

        # è·å–è§†é¢‘ä¿¡æ¯
        video_url = data["video"]["play_addr"]["url_list"][0].replace("playwm", "play")
        desc = data.get("desc", "").strip() or f"douyin_{video_id}"

        # æ›¿æ¢æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
        desc = re.sub(r'[\\/:*?"<>|]', '_', desc)

        result = {
            "url": video_url,
            "title": desc,
            "video_id": video_id
        }

        print(f"âœ… è§£ææˆåŠŸ: {desc}")
        print(f"   è§†é¢‘ID: {video_id}")

        return result

    def download_video(self, video_info: Dict, filename: Optional[str] = None) -> Path:
        """ä¸‹è½½è§†é¢‘åˆ°æœ¬åœ°"""
        if filename:
            filepath = self.output_dir / filename
        else:
            filepath = self.output_dir / f"{video_info['video_id']}.mp4"

        print(f"â¬‡ï¸  æ­£åœ¨ä¸‹è½½è§†é¢‘: {video_info['title']}")

        response = requests.get(video_info['url'], headers=HEADERS, stream=True)
        response.raise_for_status()

        # è·å–æ–‡ä»¶å¤§å°
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0

        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    if total_size > 0:
                        progress = downloaded / total_size * 100
                        print(f"\r   ä¸‹è½½è¿›åº¦: {progress:.1f}%", end="", flush=True)

        print(f"\nâœ… è§†é¢‘ä¸‹è½½å®Œæˆ: {filepath}")
        return filepath

    def get_video_duration(self, video_path: Path) -> float:
        """è·å–è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰"""
        cmd = [
            'ffprobe', '-v', 'error', '-show_entries',
            'format=duration', '-of', 'default=noprint_wrappers=1:nokey=1',
            str(video_path)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        return float(result.stdout.strip())

    def split_video(self, video_path: Path) -> List[Path]:
        """å°†é•¿è§†é¢‘åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µ"""
        duration = self.get_video_duration(video_path)
        print(f"ğŸ“Š è§†é¢‘æ€»æ—¶é•¿: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")

        # å¦‚æœè§†é¢‘æ—¶é•¿å°äºåˆ‡åˆ†é˜ˆå€¼ï¼Œç›´æ¥è¿”å›åŸè§†é¢‘
        if duration <= self.segment_duration:
            print("   è§†é¢‘è¾ƒçŸ­ï¼Œæ— éœ€åˆ‡åˆ†")
            return [video_path]

        # è®¡ç®—éœ€è¦åˆ‡åˆ†çš„ç‰‡æ®µæ•°
        num_segments = int(duration / self.segment_duration) + 1
        print(f"âœ‚ï¸  æ­£åœ¨å°†è§†é¢‘åˆ‡åˆ†ä¸º {num_segments} ä¸ªç‰‡æ®µ...")

        segments_dir = self.output_dir / "segments"
        segments_dir.mkdir(exist_ok=True)

        segment_paths = []

        for i in range(num_segments):
            start_time = i * self.segment_duration
            segment_filename = f"segment_{i:03d}.mp4"
            segment_path = segments_dir / segment_filename

            cmd = [
                'ffmpeg', '-y', '-i', str(video_path),
                '-ss', str(start_time),
                '-t', str(self.segment_duration),
                '-c', 'copy',  # æ— æŸåˆ‡åˆ†
                str(segment_path)
            ]

            subprocess.run(cmd, capture_output=True, check=True)

            # éªŒè¯ç‰‡æ®µæ˜¯å¦æœ‰æ•ˆ
            if segment_path.exists() and segment_path.stat().st_size > 0:
                segment_paths.append(segment_path)
                print(f"   ç‰‡æ®µ {i+1}/{num_segments}: {segment_filename}")

        print(f"âœ… åˆ‡åˆ†å®Œæˆï¼Œå…± {len(segment_paths)} ä¸ªç‰‡æ®µ")
        return segment_paths

    def extract_audio(self, video_path: Path) -> Path:
        """ä»è§†é¢‘æ–‡ä»¶ä¸­æå–éŸ³é¢‘"""
        audio_path = video_path.with_suffix('.mp3')

        cmd = [
            'ffmpeg', '-y', '-i', str(video_path),
            '-vn',  # ä¸è¦è§†é¢‘
            '-acodec', 'libmp3lame',
            '-q:a', '0',  # æœ€é«˜è´¨é‡
            str(audio_path)
        ]

        subprocess.run(cmd, capture_output=True, check=True)
        return audio_path

    def upload_file_for_transcription(self, file_path: Path) -> str:
        """ä¸Šä¼ æ–‡ä»¶è·å–å¯è®¿é—®çš„URLï¼ˆä½¿ç”¨é˜¿é‡Œäº‘OSSä¸´æ—¶é“¾æ¥ï¼‰"""
        # é˜¿é‡Œäº‘ç™¾ç‚¼æ”¯æŒç›´æ¥ä¸Šä¼ æœ¬åœ°æ–‡ä»¶
        # è¿™é‡Œä½¿ç”¨ file:// åè®®æˆ–ç›´æ¥ä¼ æ–‡ä»¶è·¯å¾„
        return f"file://{file_path.absolute()}"

    def transcribe_audio(self, audio_path: Path, segment_index: int = 0) -> Dict:
        """è½¬å½•å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        print(f"ğŸ¤ æ­£åœ¨è¯†åˆ«ç‰‡æ®µ {segment_index + 1}: {audio_path.name}")

        try:
            # å¯¹äºæœ¬åœ°æ–‡ä»¶ï¼Œéœ€è¦å…ˆè¯»å–å¹¶ä¸Šä¼ 
            # é˜¿é‡Œäº‘ç™¾ç‚¼æ”¯æŒç›´æ¥ä¼ æœ¬åœ°æ–‡ä»¶è·¯å¾„
            task_response = dashscope.audio.asr.Transcription.async_call(
                model=self.model,
                file_urls=[str(audio_path.absolute())],
                language_hints=['zh', 'en']
            )

            # ç­‰å¾…è½¬å½•å®Œæˆ
            transcription_response = dashscope.audio.asr.Transcription.wait(
                task=task_response.output.task_id
            )

            if transcription_response.status_code == HTTPStatus.OK:
                results = transcription_response.output.get('results', [])

                for transcription in results:
                    if 'transcription_url' in transcription:
                        url = transcription['transcription_url']
                        result = json.loads(request.urlopen(url).read().decode('utf8'))

                        if 'transcripts' in result and len(result['transcripts']) > 0:
                            text = result['transcripts'][0]['text']
                            return {
                                "segment_index": segment_index,
                                "status": "success",
                                "text": text,
                                "file": str(audio_path)
                            }

                return {
                    "segment_index": segment_index,
                    "status": "success",
                    "text": "",
                    "file": str(audio_path)
                }
            else:
                return {
                    "segment_index": segment_index,
                    "status": "error",
                    "text": "",
                    "error": transcription_response.output.get('message', 'æœªçŸ¥é”™è¯¯'),
                    "file": str(audio_path)
                }

        except Exception as e:
            return {
                "segment_index": segment_index,
                "status": "error",
                "text": "",
                "error": str(e),
                "file": str(audio_path)
            }

    def transcribe_video_url(self, video_url: str) -> str:
        """ç›´æ¥é€šè¿‡è§†é¢‘URLè¿›è¡Œè½¬å½•ï¼ˆé€‚ç”¨äºçŸ­è§†é¢‘ï¼‰"""
        print("ğŸ¤ æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«...")

        try:
            task_response = dashscope.audio.asr.Transcription.async_call(
                model=self.model,
                file_urls=[video_url],
                language_hints=['zh', 'en']
            )

            transcription_response = dashscope.audio.asr.Transcription.wait(
                task=task_response.output.task_id
            )

            if transcription_response.status_code == HTTPStatus.OK:
                for transcription in transcription_response.output['results']:
                    url = transcription['transcription_url']
                    result = json.loads(request.urlopen(url).read().decode('utf8'))

                    if 'transcripts' in result and len(result['transcripts']) > 0:
                        return result['transcripts'][0]['text']

                return "æœªè¯†åˆ«åˆ°æ–‡æœ¬å†…å®¹"
            else:
                raise Exception(f"è½¬å½•å¤±è´¥: {transcription_response.output.message}")

        except Exception as e:
            raise Exception(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")

    def split_video_for_ocr(self, video_path: Path) -> List[Tuple[Path, float]]:
        """å°†è§†é¢‘åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µç”¨äº OCR å¹¶è¡Œå¤„ç†

        Returns:
            [(segment_path, time_offset), ...]
        """
        duration = self.get_video_duration(video_path)
        print(f"ğŸ“Š è§†é¢‘æ€»æ—¶é•¿: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")

        if duration <= OCR_SEGMENT_DURATION:
            print("   è§†é¢‘è¾ƒçŸ­ï¼Œæ— éœ€åˆ‡åˆ†")
            return [(video_path, 0.0)]

        num_segments = int(duration / OCR_SEGMENT_DURATION) + 1
        print(f"âœ‚ï¸  æ­£åœ¨å°†è§†é¢‘åˆ‡åˆ†ä¸º {num_segments} ä¸ªç‰‡æ®µï¼ˆæ¯æ®µ {OCR_SEGMENT_DURATION} ç§’ï¼‰...")

        segments_dir = self.output_dir / "ocr_segments"
        segments_dir.mkdir(exist_ok=True)

        segment_info = []
        for i in range(num_segments):
            start_time = i * OCR_SEGMENT_DURATION
            segment_path = segments_dir / f"segment_{i:03d}.mp4"

            cmd = [
                'ffmpeg', '-y', '-i', str(video_path),
                '-ss', str(start_time),
                '-t', str(OCR_SEGMENT_DURATION),
                '-c', 'copy',
                str(segment_path)
            ]
            subprocess.run(cmd, capture_output=True, check=True)

            if segment_path.exists() and segment_path.stat().st_size > 0:
                segment_info.append((segment_path, start_time))
                print(f"   ç‰‡æ®µ {i+1}/{num_segments}: {segment_path.name}")

        print(f"âœ… åˆ‡åˆ†å®Œæˆï¼Œå…± {len(segment_info)} ä¸ªç‰‡æ®µ")
        return segment_info

    def process_video_ocr(self, video_path: Path) -> Tuple[str, List[Dict]]:
        """ä½¿ç”¨ OCR æå–è§†é¢‘ç¡¬å­—å¹•ï¼ˆæ”¯æŒå¹¶è¡Œå¤„ç†ï¼‰"""
        print("\nğŸ”¤ æ­£åœ¨ä½¿ç”¨ OCR æå–ç¡¬å­—å¹•...")

        # 1. åˆ‡åˆ†è§†é¢‘
        segments = self.split_video_for_ocr(video_path)

        # 2. å‡†å¤‡å¹¶è¡Œä»»åŠ¡å‚æ•°
        tasks = [
            (str(seg_path), idx, time_offset, 'ch', self.ocr_engine.subtitle_area if self._ocr_engine else None)
            for idx, (seg_path, time_offset) in enumerate(segments)
        ]

        # 3. å¹¶è¡Œå¤„ç†
        num_workers = min(OCR_PARALLEL_WORKERS, len(segments))
        print(f"\nğŸš€ ä½¿ç”¨ {num_workers} è¿›ç¨‹å¹¶è¡Œå¤„ç†...")

        all_subtitles = []
        if len(segments) == 1:
            # å•ç‰‡æ®µç›´æ¥å¤„ç†
            _, subs = _process_segment_ocr(tasks[0])
            all_subtitles = subs
        else:
            # å¤šç‰‡æ®µå¹¶è¡Œå¤„ç†
            with ProcessPoolExecutor(max_workers=num_workers) as executor:
                results = list(executor.map(_process_segment_ocr, tasks))

            # æŒ‰ç‰‡æ®µé¡ºåºåˆå¹¶
            results.sort(key=lambda x: x[0])
            for _, subs in results:
                all_subtitles.extend(subs)

        print(f"\nâœ… è¯†åˆ«å®Œæˆï¼Œå…±æå– {len(all_subtitles)} æ¡å­—å¹•")

        # 4. è½¬æ¢ä¸ºçº¯æ–‡æœ¬
        full_text = self.ocr_engine.subtitles_to_text(all_subtitles)

        # 5. ä¿å­˜ SRT æ–‡ä»¶
        srt_content = self.ocr_engine.subtitles_to_srt(all_subtitles)
        srt_file = self.output_dir / f"{video_path.stem}.srt"
        with open(srt_file, 'w', encoding='utf-8') as f:
            f.write(srt_content)
        print(f"ğŸ“„ SRT å­—å¹•å·²ä¿å­˜: {srt_file}")

        # æ¸…ç†ä¸´æ—¶ç‰‡æ®µ
        segments_dir = self.output_dir / "ocr_segments"
        if segments_dir.exists():
            shutil.rmtree(segments_dir)
            print("ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶ç‰‡æ®µæ–‡ä»¶")

        # æ„å»ºç»“æœ
        results = [{
            "segment_index": 0,
            "status": "success",
            "text": full_text,
            "subtitles": all_subtitles,
            "file": str(video_path)
        }]

        return full_text, results

    def process_long_video(
        self,
        video_path: Path,
        parallel: bool = True
    ) -> Tuple[str, List[Dict]]:
        """å¤„ç†é•¿è§†é¢‘ï¼šåˆ‡åˆ†ã€æå–éŸ³é¢‘/å­—å¹•ã€è¯†åˆ«ã€åˆå¹¶"""

        # OCR æ¨¡å¼ï¼šç›´æ¥å¤„ç†æ•´ä¸ªè§†é¢‘
        if self.use_ocr:
            return self.process_video_ocr(video_path)

        # ASR æ¨¡å¼ï¼šåˆ‡åˆ†ã€æå–éŸ³é¢‘ã€è¯­éŸ³è¯†åˆ«
        # 1. åˆ‡åˆ†è§†é¢‘
        segments = self.split_video(video_path)

        # 2. æå–æ¯ä¸ªç‰‡æ®µçš„éŸ³é¢‘
        print("\nğŸ”Š æ­£åœ¨æå–éŸ³é¢‘...")
        audio_files = []
        for i, segment in enumerate(segments):
            audio_path = self.extract_audio(segment)
            audio_files.append((i, audio_path))
            print(f"   éŸ³é¢‘ {i+1}/{len(segments)}: {audio_path.name}")

        # 3. å¹¶è¡Œæˆ–ä¸²è¡Œè½¬å½•
        print("\nğŸ¤ æ­£åœ¨è¿›è¡Œè¯­éŸ³è¯†åˆ«...")
        results = []

        if parallel and len(audio_files) > 1:
            with ThreadPoolExecutor(max_workers=MAX_PARALLEL_TRANSCRIPTIONS) as executor:
                futures = {
                    executor.submit(self.transcribe_audio, audio_path, idx): idx
                    for idx, audio_path in audio_files
                }

                for future in as_completed(futures):
                    result = future.result()
                    results.append(result)
                    status = "âœ…" if result["status"] == "success" else "âŒ"
                    print(f"   {status} ç‰‡æ®µ {result['segment_index'] + 1} è¯†åˆ«å®Œæˆ")
        else:
            for idx, audio_path in audio_files:
                result = self.transcribe_audio(audio_path, idx)
                results.append(result)
                status = "âœ…" if result["status"] == "success" else "âŒ"
                print(f"   {status} ç‰‡æ®µ {idx + 1} è¯†åˆ«å®Œæˆ")

        # 4. æŒ‰é¡ºåºåˆå¹¶ç»“æœ
        results.sort(key=lambda x: x["segment_index"])

        full_text_parts = []
        for result in results:
            if result["status"] == "success" and result["text"]:
                full_text_parts.append(result["text"])

        full_text = "\n\n".join(full_text_parts)

        return full_text, results

    def process(
        self,
        share_link: str,
        force_local: bool = False,
        keep_files: bool = False
    ) -> Dict:
        """å®Œæ•´å¤„ç†æµç¨‹"""

        # 1. è§£æåˆ†äº«é“¾æ¥
        video_info = self.parse_share_url(share_link)

        # 2. OCR æ¨¡å¼å¿…é¡»æœ¬åœ°å¤„ç†
        if self.use_ocr:
            force_local = True

        # 3. åˆ¤æ–­æ˜¯å¦éœ€è¦æœ¬åœ°å¤„ç†
        # å…ˆå°è¯•ç›´æ¥URLè¯†åˆ«ï¼ˆé€‚ç”¨äºçŸ­è§†é¢‘ï¼Œä»…ASRæ¨¡å¼ï¼‰
        if not force_local and not self.use_ocr:
            try:
                print("\nğŸš€ å°è¯•ç›´æ¥åœ¨çº¿è¯†åˆ«...")
                text = self.transcribe_video_url(video_info['url'])

                return {
                    "status": "success",
                    "video_id": video_info["video_id"],
                    "title": video_info["title"],
                    "text": text,
                    "method": "online_asr",
                    "segments": []
                }
            except Exception as e:
                print(f"âš ï¸  åœ¨çº¿è¯†åˆ«å¤±è´¥: {e}")
                print("   åˆ‡æ¢åˆ°æœ¬åœ°å¤„ç†æ¨¡å¼...")

        # 4. æœ¬åœ°å¤„ç†æµç¨‹
        method = "local_ocr" if self.use_ocr else "local_asr"
        print(f"\nğŸ“¥ å¯åŠ¨æœ¬åœ°å¤„ç†æ¨¡å¼ ({method})...")

        # ä¸‹è½½è§†é¢‘
        video_path = self.download_video(video_info)

        # å¤„ç†é•¿è§†é¢‘
        full_text, segment_results = self.process_long_video(video_path)

        # ä¿å­˜ç»“æœ
        result = {
            "status": "success",
            "video_id": video_info["video_id"],
            "title": video_info["title"],
            "text": full_text,
            "method": "local",
            "segments": segment_results,
            "output_dir": str(self.output_dir)
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        result_file = self.output_dir / f"{video_info['video_id']}_result.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

        text_file = self.output_dir / f"{video_info['video_id']}_transcript.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(f"æ ‡é¢˜: {video_info['title']}\n")
            f.write(f"è§†é¢‘ID: {video_info['video_id']}\n")
            f.write(f"{'='*50}\n\n")
            f.write(full_text)

        print(f"\nğŸ“ è½¬å½•ç»“æœå·²ä¿å­˜: {text_file}")

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if not keep_files:
            segments_dir = self.output_dir / "segments"
            if segments_dir.exists():
                shutil.rmtree(segments_dir)
                print("ğŸ§¹ å·²æ¸…ç†ä¸´æ—¶ç‰‡æ®µæ–‡ä»¶")

        return result


def process_local_video(
    video_path: str,
    api_key: str = "",
    model: str = DEFAULT_MODEL,
    output_dir: Optional[str] = None,
    segment_duration: int = DEFAULT_SEGMENT_DURATION,
    use_ocr: bool = False,
    ocr_gpu: bool = False
) -> Dict:
    """å¤„ç†æœ¬åœ°è§†é¢‘æ–‡ä»¶"""

    video_path = Path(video_path)
    if not video_path.exists():
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")

    processor = DouyinVideoProcessor(
        api_key=api_key,
        model=model,
        output_dir=output_dir or str(video_path.parent),
        segment_duration=segment_duration,
        use_ocr=use_ocr,
        ocr_gpu=ocr_gpu
    )

    print(f"ğŸ“¹ å¤„ç†æœ¬åœ°è§†é¢‘: {video_path}")

    full_text, segment_results = processor.process_long_video(video_path)

    result = {
        "status": "success",
        "source": str(video_path),
        "text": full_text,
        "segments": segment_results,
        "output_dir": str(processor.output_dir)
    }

    # ä¿å­˜ç»“æœ
    result_file = processor.output_dir / f"{video_path.stem}_result.json"
    with open(result_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    text_file = processor.output_dir / f"{video_path.stem}_transcript.txt"
    with open(text_file, 'w', encoding='utf-8') as f:
        f.write(f"æºæ–‡ä»¶: {video_path}\n")
        f.write(f"{'='*50}\n\n")
        f.write(full_text)

    print(f"\nğŸ“ è½¬å½•ç»“æœå·²ä¿å­˜: {text_file}")

    return result


def main():
    parser = argparse.ArgumentParser(
        description="æŠ–éŸ³é•¿è§†é¢‘å¤„ç†å·¥å…· - æ”¯æŒ ASR è¯­éŸ³è¯†åˆ«å’Œ OCR ç¡¬å­—å¹•æå–",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # ASR æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰- è¯­éŸ³è¯†åˆ«
  python douyin_video_processor.py "æŠ–éŸ³åˆ†äº«é“¾æ¥"

  # OCR æ¨¡å¼ - æå–è§†é¢‘ç¡¬å­—å¹•ï¼ˆæ›´å‡†ç¡®ï¼Œéœ€è¦è§†é¢‘æœ‰å­—å¹•ï¼‰
  python douyin_video_processor.py "æŠ–éŸ³åˆ†äº«é“¾æ¥" --ocr

  # OCR æ¨¡å¼ + GPU åŠ é€Ÿ
  python douyin_video_processor.py "æŠ–éŸ³åˆ†äº«é“¾æ¥" --ocr --ocr-gpu

  # å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ¨¡å¼å¤„ç†ï¼ˆASRæ¨¡å¼ï¼‰
  python douyin_video_processor.py "æŠ–éŸ³åˆ†äº«é“¾æ¥" --force-local

  # å¤„ç†æœ¬åœ°è§†é¢‘æ–‡ä»¶
  python douyin_video_processor.py --local /path/to/video.mp4
  python douyin_video_processor.py --local /path/to/video.mp4 --ocr

  # æŒ‡å®šè¾“å‡ºç›®å½•
  python douyin_video_processor.py "æŠ–éŸ³åˆ†äº«é“¾æ¥" -o ./output

  # è‡ªå®šä¹‰åˆ‡åˆ†æ—¶é•¿ï¼ˆç§’ï¼Œä»…ASRæ¨¡å¼ï¼‰
  python douyin_video_processor.py "æŠ–éŸ³åˆ†äº«é“¾æ¥" --segment-duration 180

ç¯å¢ƒå˜é‡:
  DASHSCOPE_API_KEY: é˜¿é‡Œäº‘ç™¾ç‚¼APIå¯†é’¥ï¼ˆASRæ¨¡å¼éœ€è¦ï¼ŒOCRæ¨¡å¼ä¸éœ€è¦ï¼‰

æ¨¡å¼å¯¹æ¯”:
  ASR æ¨¡å¼: è¯­éŸ³è½¬æ–‡å­—ï¼Œé€‚ç”¨äºæ‰€æœ‰è§†é¢‘ï¼Œéœ€è¦ API å¯†é’¥
  OCR æ¨¡å¼: ç¡¬å­—å¹•è¯†åˆ«ï¼Œæ›´å‡†ç¡®ä½†éœ€è¦è§†é¢‘æœ‰å­—å¹•ï¼Œæ— éœ€ API å¯†é’¥
        """
    )

    parser.add_argument(
        "input",
        nargs="?",
        help="æŠ–éŸ³åˆ†äº«é“¾æ¥æˆ–æœ¬åœ°è§†é¢‘è·¯å¾„"
    )

    parser.add_argument(
        "--local", "-l",
        action="store_true",
        help="å¤„ç†æœ¬åœ°è§†é¢‘æ–‡ä»¶"
    )

    parser.add_argument(
        "--ocr",
        action="store_true",
        help="ä½¿ç”¨ OCR æ¨¡å¼æå–ç¡¬å­—å¹•ï¼ˆéœ€è¦è§†é¢‘æœ‰å­—å¹•ï¼Œæ— éœ€APIå¯†é’¥ï¼‰"
    )

    parser.add_argument(
        "--ocr-gpu",
        action="store_true",
        help="OCR æ¨¡å¼ä½¿ç”¨ GPU åŠ é€Ÿï¼ˆéœ€è¦å®‰è£… paddlepaddle-gpuï¼‰"
    )

    parser.add_argument(
        "--force-local", "-f",
        action="store_true",
        help="å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°ä¸‹è½½æ¨¡å¼ï¼ˆè·³è¿‡åœ¨çº¿è¯†åˆ«ï¼Œä»…ASRæ¨¡å¼ï¼‰"
    )

    parser.add_argument(
        "--output", "-o",
        type=str,
        default=None,
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨ä¸´æ—¶ç›®å½•ï¼‰"
    )

    parser.add_argument(
        "--segment-duration", "-s",
        type=int,
        default=DEFAULT_SEGMENT_DURATION,
        help=f"è§†é¢‘åˆ‡åˆ†æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ {DEFAULT_SEGMENT_DURATION}ï¼Œä»…ASRæ¨¡å¼"
    )

    parser.add_argument(
        "--model", "-m",
        type=str,
        default=DEFAULT_MODEL,
        help=f"è¯­éŸ³è¯†åˆ«æ¨¡å‹ï¼Œé»˜è®¤ {DEFAULT_MODEL}ï¼Œä»…ASRæ¨¡å¼"
    )

    parser.add_argument(
        "--keep-files", "-k",
        action="store_true",
        help="ä¿ç•™ä¸­é—´æ–‡ä»¶ï¼ˆåˆ‡åˆ†çš„ç‰‡æ®µå’ŒéŸ³é¢‘ï¼‰"
    )

    parser.add_argument(
        "--info-only",
        action="store_true",
        help="ä»…è·å–è§†é¢‘ä¿¡æ¯ï¼Œä¸è¿›è¡Œä¸‹è½½å’Œè¯†åˆ«"
    )

    args = parser.parse_args()

    # æ£€æŸ¥APIå¯†é’¥ï¼ˆä»…ASRæ¨¡å¼éœ€è¦ï¼‰
    api_key = os.getenv('DASHSCOPE_API_KEY', '')
    if not args.ocr and not api_key and not args.info_only:
        print("âŒ é”™è¯¯: ASR æ¨¡å¼éœ€è¦è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEY")
        print("   è¯·è®¾ç½®é˜¿é‡Œäº‘ç™¾ç‚¼APIå¯†é’¥:")
        print("   export DASHSCOPE_API_KEY='your-api-key'")
        print("")
        print("   æˆ–è€…ä½¿ç”¨ OCR æ¨¡å¼ï¼ˆæ— éœ€APIå¯†é’¥ï¼‰:")
        print("   python douyin_video_processor.py \"é“¾æ¥\" --ocr")
        sys.exit(1)

    if not args.input:
        parser.print_help()
        sys.exit(1)

    try:
        if args.local:
            # å¤„ç†æœ¬åœ°è§†é¢‘
            result = process_local_video(
                video_path=args.input,
                api_key=api_key,
                model=args.model,
                output_dir=args.output,
                segment_duration=args.segment_duration,
                use_ocr=args.ocr,
                ocr_gpu=args.ocr_gpu
            )
        else:
            processor = DouyinVideoProcessor(
                api_key=api_key,
                model=args.model,
                output_dir=args.output,
                segment_duration=args.segment_duration,
                use_ocr=args.ocr,
                ocr_gpu=args.ocr_gpu
            )

            if args.info_only:
                # ä»…è·å–è§†é¢‘ä¿¡æ¯
                video_info = processor.parse_share_url(args.input)
                print("\nğŸ“‹ è§†é¢‘ä¿¡æ¯:")
                print(json.dumps(video_info, ensure_ascii=False, indent=2))
            else:
                # å®Œæ•´å¤„ç†æµç¨‹
                result = processor.process(
                    share_link=args.input,
                    force_local=args.force_local or args.ocr,
                    keep_files=args.keep_files
                )

                print("\n" + "="*60)
                print("ğŸ“œ è¯†åˆ«ç»“æœ:")
                print("="*60)
                print(result["text"])
                print("="*60)

        print("\nâœ… å¤„ç†å®Œæˆ!")

    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
